# ğŸ“‘ Quellen zum Vortrag *AI_Wingman*

## âš™ï¸ Inferenzierung von LLMs

### ğŸ–¥ï¸ Lokale Inferenzierung auf eigener Hardware

ğŸ”— [LM Studio](https://lmstudio.ai/)
ğŸ”— [Ollama](https://github.com/ollama/ollama)

> âœ¨ Damit kannst du Modelle direkt auf deinem eigenen Rechner ausprobieren â€“ perfekt zum Experimentieren ohne Cloud-AbhÃ¤ngigkeit.

### ğŸ¢ Inferenzierung auf Server-Hardware

Wir verwenden **vLLM**:
ğŸ”— [vLLM Projekt auf GitHub](https://github.com/vllm-project/vllm)

> âš¡ vLLM sorgt fÃ¼r hochperformante Inferenzierung und effiziente Nutzung deiner Server-Ressourcen.

***

## ğŸ¯ Auswahl eines LLMs

- ğŸ“Š Benchmarks: [LiveBench](https://livebench.ai/#/)
- ğŸ“‚ Modelle beziehen: [Hugging Face](https://huggingface.co/)
- ğŸ¤– Derzeit im Einsatz: [Qwen3-235B-A22B-GPTQ-Int4](https://huggingface.co/Qwen/Qwen3-235B-A22B-GPTQ-Int4)

> ğŸ” Benchmarks helfen bei der Auswahl. Hugging Face ist die zentrale Plattform fÃ¼r open Source Modelle.

***

## ğŸ› ï¸ Plugins fÃ¼r die IDE

### ğŸ’¡ VS Code

- [Continue.dev](https://www.continue.dev/)

### ğŸ§© JetBrains IDEs

- [Proxy-AI](https://github.com/carlrobertoh/ProxyAI)
- Coming Soon: [JetBrains AI Assistant](https://www.jetbrains.com/ai-assistant/)

> ğŸ’¡ JetBrains zieht nach â€“ bald auch mit Nutzung eigener LLMS via OpenAI-API.

***

## ğŸŒ Zugriff Ã¼ber den Browser

- [OpenWeb-UI](https://github.com/open-webui/open-webui)

> ğŸŒ Damit bekommst du ein schickes Web-Interface und kannst eigene LLMS via OpenAI-API anbinden.

***

## ğŸš€ Booste deine LLMS mit zusÃ¤tzlichem Wissen und Funktionen

### ğŸ“š RAG (Retrieval Augmented Generation)

- [Intro zu RAG](https://www.ionos.de/digitalguide/server/knowhow/retrieval-augmented-generation/)

> ğŸ“– RAG erweitert dein LLM mit eigenem Wissen â€“ nutze Dokumente, Datenbanken oder Firmenwissen fÃ¼r bessere Antworten.

### ğŸ”Œ TOOLS mit MCP (Model Context Protocol)

- [Model Context Protocol auf GitHub](https://github.com/modelcontextprotocol)
- Ã–ffentliche MCP-Server: [mcp.so](https://mcp.so)

> ğŸ§© Mit MCP kannst du Tools, APIs und externe Datenquellen direkt ins LLM integrieren.

***
