# 📑 Quellen zum Vortrag *AI_Wingman*

## ⚙️ Inferenzierung von LLMs

### 🖥️ Lokale Inferenzierung auf eigener Hardware

🔗 [LM Studio](https://lmstudio.ai/)
🔗 [Ollama](https://github.com/ollama/ollama)

> ✨ Damit kannst du Modelle direkt auf deinem eigenen Rechner ausprobieren – perfekt zum Experimentieren ohne Cloud-Abhängigkeit.

### 🏢 Inferenzierung auf Server-Hardware

Wir verwenden **vLLM**:
🔗 [vLLM Projekt auf GitHub](https://github.com/vllm-project/vllm)

> ⚡ vLLM sorgt für hochperformante Inferenzierung und effiziente Nutzung deiner Server-Ressourcen.

***

## 🎯 Auswahl eines LLMs

- 📊 Benchmarks: [LiveBench](https://livebench.ai/#/)
- 📂 Modelle beziehen: [Hugging Face](https://huggingface.co/)
- 🤖 Derzeit im Einsatz: [Qwen3-235B-A22B-GPTQ-Int4](https://huggingface.co/Qwen/Qwen3-235B-A22B-GPTQ-Int4)

> 🔍 Benchmarks helfen bei der Auswahl. Hugging Face ist die zentrale Plattform für open Source Modelle.

***

## 🛠️ Plugins für die IDE

### 💡 VS Code

- [Continue.dev](https://www.continue.dev/)

### 🧩 JetBrains IDEs

- [Proxy-AI](https://github.com/carlrobertoh/ProxyAI)
- Coming Soon: [JetBrains AI Assistant](https://www.jetbrains.com/ai-assistant/)

> 💡 JetBrains zieht nach – bald auch mit Nutzung eigener LLMS via OpenAI-API.

***

## 🌐 Zugriff über den Browser

- [OpenWeb-UI](https://github.com/open-webui/open-webui)

> 🌍 Damit bekommst du ein schickes Web-Interface und kannst eigene LLMS via OpenAI-API anbinden.

***

## 🚀 Booste deine LLMS mit zusätzlichem Wissen und Funktionen

### 📚 RAG (Retrieval Augmented Generation)

- [Intro zu RAG](https://www.ionos.de/digitalguide/server/knowhow/retrieval-augmented-generation/)

> 📖 RAG erweitert dein LLM mit eigenem Wissen – nutze Dokumente, Datenbanken oder Firmenwissen für bessere Antworten.

### 🔌 TOOLS mit MCP (Model Context Protocol)

- [Model Context Protocol auf GitHub](https://github.com/modelcontextprotocol)
- Öffentliche MCP-Server: [mcp.so](https://mcp.so)

> 🧩 Mit MCP kannst du Tools, APIs und externe Datenquellen direkt ins LLM integrieren.

***
